{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#0.Lab 01 - RDD vs DataFrame\n",
        "\n",
        "Dalam latihan ini kita akan melihat perbedaan fungsi dan operasi antara RDD dan DataFrame, yaitu filtering dan agregasi."
      ],
      "metadata": {
        "id": "OCH4pZBocq0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instal pyspark"
      ],
      "metadata": {
        "id": "fQeTdO4cX6h9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMXOGEjGU0VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92207bc-89f7-4899-9ce8-f99c1d05251d"
      },
      "source": [
        "%pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/317.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import package yang dibutuhkan"
      ],
      "metadata": {
        "id": "Bggy7DeBX-1c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0CyPWlHU5oL"
      },
      "source": [
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create spark session"
      ],
      "metadata": {
        "id": "3s0yTjp_X2JP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFeguv1rU6lq"
      },
      "source": [
        "spark = SparkSession.builder.appName('RDD & DataFrame').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create\n",
        "\n"
      ],
      "metadata": {
        "id": "sbnDZElFYM5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "listData = [(\"Banten\",1222258,\"Lebak\"),\n",
        "(\"Banten\",1175148,\"Pandeglang\"),\n",
        "(\"Banten\",1435003,\"Serang\"),\n",
        "(\"Banten\",2619803,\"Tangerang\"),\n",
        "(\"DKI Jakarta\",23340,\"Kepulauan Seribu\"),\n",
        "(\"DKI Jakarta\",2185711,\"Jakarta Selatan\"),\n",
        "(\"DKI Jakarta\",2843816,\"Jakarta Timur\"),\n",
        "(\"DKI Jakarta\",914182,\"Jakarta Pusat\"),\n",
        "(\"DKI Jakarta\",2463560,\"Jakarta Barat\"),\n",
        "(\"DKI Jakarta\",1747315,\"Jakarta Utara\")]\n",
        "\n",
        "listData"
      ],
      "metadata": {
        "id": "pGeWMd7qFqiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create RDD\n",
        "\n",
        "Create spark RDD dengan menggunakan fungsi `sparkContext.parallelize()`"
      ],
      "metadata": {
        "id": "PqiNheFLr0ST"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VUblqaN4kbP"
      },
      "source": [
        "dataRDD = spark.sparkContext.parallelize(listData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tampilkan hasilnya dengan perintah `collect`\n",
        "\n",
        "**Perhatikan** bahwa ketika kita memanggil perintah `collect()` terhadap sebuah RDD ataupun Dataset, **seluruh data dalam RDD tersebut akan dikirim ke node di mana driver berada**. Oleh karena itu perlu dipertimbangkan ukuran data sebelum memanggil fungsi `collect`."
      ],
      "metadata": {
        "id": "YhfZPsP3w5ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataRDD.collect()"
      ],
      "metadata": {
        "id": "TlcgICGqxCH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DataFrame\n",
        "Create DataFrame dengan menggunakan fungsi `createDataFrame()`"
      ],
      "metadata": {
        "id": "roHJPpLMr5z5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGHSciY25h0d"
      },
      "source": [
        "dataDF = spark.createDataFrame(listData, [\"province\", \"pop\",\"district\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tampilkan hasilnya dengan perintah `show`"
      ],
      "metadata": {
        "id": "UE6tIFDzxGeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataDF.show()"
      ],
      "metadata": {
        "id": "IPeR7SrrxJyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter data RDD\n",
        "\n",
        "Untuk melakukan filtering RDD, kita menggunakan fungsi transformasi `filter`, dengan parameter berupa ekspresi `lambda`.\n",
        "\n",
        "RDD tidak memiliki skema, sehingga kita mengakses baris dan kolom dengan menggunakan indeks.\n",
        "\n",
        "Perhatikan bahwa perintah-perintah ini tidak langsung dijalankan oleh Spark, meskipun kita meng-execute cell yang bersangkutan."
      ],
      "metadata": {
        "id": "LdYZIRwftOw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rddf = dataRDD.filter(lambda x: x[0] == 'Banten')"
      ],
      "metadata": {
        "id": "5WyfOAWduMo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rddf.collect()"
      ],
      "metadata": {
        "id": "Q1iyZXw_vgfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering DataFrame\n",
        "\n",
        "Untuk melakukan filtering, digunakan fungsi `filter`.\n",
        "\n",
        "Karena DataFrame memiliki skema, maka kita dapat menggunakan nama kolom untuk melakukan filtering"
      ],
      "metadata": {
        "id": "mBiVzq29wAVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dff = dataDF.filter(dataDF['province'] == 'Banten')"
      ],
      "metadata": {
        "id": "9jDP0vOwvGse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tampilkan hasilnya dengan perintah `show`"
      ],
      "metadata": {
        "id": "qBCdRtoevjiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dff.show()"
      ],
      "metadata": {
        "id": "RS1F_-FRvj1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group by dan Agregasi"
      ],
      "metadata": {
        "id": "z9BUd7bkr-1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RDD\n",
        "\n",
        "Untuk melakukan group by dan agregasi pada RDD, kita menggunakan fungsi transformasi `map` dan `reduceByKey`, dengan mengirimkan fungsi `lambda` sebagai parameter operasinya.\n",
        "\n",
        "- `map` menerapkan ekspresi `lambda` pada **setiap element x** RDD\n",
        "- `reduceByKey` melakukan operasi agregasi berdasar key tertentu sesuai ekspresi `lambda`"
      ],
      "metadata": {
        "id": "8H2TrkMIY7N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map 1       : tambahkan integer 1 di tiap row untuk melakukan count\n",
        "# reduceByKey : jumlahkan (sum) kolom populasi dan count\n",
        "# map 2       : hitung rata-rata dengan operasi : sum(populasi)/count\n",
        "\n",
        "popRDD = (dataRDD\n",
        "           .map(lambda x: (x[0], (x[1], 1)))\n",
        "           .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "           .map(lambda x: (x[0], x[1][0]/x[1][1])))"
      ],
      "metadata": {
        "id": "jc4mjEUdYxk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tampilkan hasilnya"
      ],
      "metadata": {
        "id": "NNZjZdbbY4rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "popRDD.collect()"
      ],
      "metadata": {
        "id": "nN6mcrp0Y3DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataFrame"
      ],
      "metadata": {
        "id": "nXJ-1pHDbcm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groupby dan agregat dengan menggunakan fungsi `groupBy` dan `agg`."
      ],
      "metadata": {
        "id": "TKq6t6Tfb5by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by province name and average population\n",
        "avgDF = dataDF.groupBy(\"province\").agg(avg(\"pop\"))"
      ],
      "metadata": {
        "id": "qP52TuuqcDJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tampilkan hasilnya"
      ],
      "metadata": {
        "id": "WHGaT6cIcE8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the results\n",
        "avgDF.show()"
      ],
      "metadata": {
        "id": "JTKnSA9wcF39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiotjcgbz9gv"
      },
      "source": [
        "#1.Lab 02 - Membuat DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W-ad4trUN7WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrame dapat dibuat dengan banyak cara, di antaranya :\n",
        "- Dari python object, misalnya array/list, dictionary, pandas dataframe, dll\n",
        "- Dari file : csv, json, dll\n",
        "- Dari HDFS\n",
        "- Dari RDD\n",
        "- dll."
      ],
      "metadata": {
        "id": "R4puMNX2WfRN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOf-4dxAgOsv"
      },
      "source": [
        "##1.1. Create From Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScgatWyqepcH"
      },
      "source": [
        "mydata = (('DKI JAKARTA',15328),\n",
        "('JAWA BARAT',1320),\n",
        "('JAWA TENGAH',1030),\n",
        "('DI YOGYAKARTA',1174),\n",
        "('JAWA TIMUR',813),\n",
        "('BANTEN',1237))\n",
        "\n",
        "df_from_array = spark.createDataFrame(mydata).toDF(\"province\", \"density\")\n",
        "\n",
        "df_from_array.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYN1nYAiWx6u"
      },
      "source": [
        "##1.2. Create from Pandas DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dowload file"
      ],
      "metadata": {
        "id": "H4ksYmW7ZCXF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSEQn5OJ4vnK"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/penduduk2015.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create pandas dataframe dari file csv tersebut"
      ],
      "metadata": {
        "id": "AzgGjBoCZEqP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34owT-3uAGca"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pddf = pd.read_csv('penduduk2015.csv')\n",
        "pddf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ubah ke Spark dataframe"
      ],
      "metadata": {
        "id": "3P8yoarCZL5o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Hyq3iuO-RF"
      },
      "source": [
        "df_from_pandas = spark.createDataFrame(pddf)\n",
        "df_from_pandas.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKco8E9FgidD"
      },
      "source": [
        "##1.3. Create from csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita juga bisa me-load langsung file csv tersebut ke Spark dataframe"
      ],
      "metadata": {
        "id": "5IwfM7QBZRlS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ3yUkvjlKqN"
      },
      "source": [
        "df_from_csv = spark.read.csv(\"penduduk2015.csv\", header=True)\n",
        "df_from_csv.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n83qkTxzrrma"
      },
      "source": [
        "##1.4. Create from JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_NWvf1E52v2"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/penduduk2015.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTcgb1HEevnh"
      },
      "source": [
        "Tampilkan isi file dengan perintah `cat`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nI8eugSjORM"
      },
      "source": [
        "!cat penduduk2015.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk membaca multiline JSON, set parameter `multiline` = True"
      ],
      "metadata": {
        "id": "cmMxONptZmPU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhVBi_ndruRA"
      },
      "source": [
        "dfj = spark.read.json(\"penduduk2015.json\", multiLine=True)\n",
        "dfj.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzpuNpb8myW3"
      },
      "source": [
        "#2.Lab 03 - Explorasi DataFrame\n",
        "\n",
        "Dalam latihan ini kita akan mencoba berbagai operasi pada Spark DataFrame untuk melakukan eksplorasi data.\n",
        "\n",
        "Kita akan menggunakan data kepadatan penduduk per propinsi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qO8foHlWexn"
      },
      "source": [
        "#!wget https://www.dropbox.com/s/4ozzaa2yvy2kkza/indonesia2013-2015.csv\n",
        "!wget https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/indonesia2013-2015.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita gunakan magic command untuk melihat ukuran dan isi file (karena file kita cukup kecil)."
      ],
      "metadata": {
        "id": "3qD7ifpcl4ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -al indonesia2013-2015.csv"
      ],
      "metadata": {
        "id": "4YKCOaiwmKO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cat indonesia2013-2015.csv"
      ],
      "metadata": {
        "id": "kEX7VuFZl498"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Karena data yang kita load sudah bersih, kita akan set inferSchema = True agar Spark menyesuaikan tipe kolom dengan datanya."
      ],
      "metadata": {
        "id": "Iw-Y0fIMl29B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tQqD2VYm7ZR"
      },
      "source": [
        "df = spark.read.csv(\"indonesia2013-2015.csv\",header=True,inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbwg-RH5m1JG"
      },
      "source": [
        "###2.1 Melihat sekilas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTGwnupfKBMU"
      },
      "source": [
        "####Menampilkan beberapa baris\n",
        "\n",
        "Biasanya kita menampilkan beberapa baris data untuk mengecek format dan konten dataframe yang kita buat.\n",
        "\n",
        "Untuk menampilkan beberapa baris dari dataframe, kita bisa gunakan perintah ``show(n)`` untuk menampilkan n baris pertama, atau ``first()`` untuk menampilkan 1 baris pertama saja."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-ccShKgnGBw"
      },
      "source": [
        "df.show(5)\n",
        "df.first()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX7rzHbOQRyv"
      },
      "source": [
        "####Menampilkan jumlah kolom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbUMZTJ8QSYF"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkyOlEULP_g-"
      },
      "source": [
        "####Menampilkan total records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtSRF5xFQBmc"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99-M00F5nKdO"
      },
      "source": [
        "####Menampilkan skema\n",
        "\n",
        "Untuk menampilkan skema dataframe, gunakan fungsi `printSchema()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOlh-X3nnL62"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_rKrAAenlC0"
      },
      "source": [
        "Akses atribut `columns` untuk menampilkan list nama kolom\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8el4g04nnII"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Akses atribut `dtypes` untuk menampilkan list nama kolom beserta data type masing-masing kolom tersebut"
      ],
      "metadata": {
        "id": "juHaBiLq_ldO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49CkcSSNnuRH"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpPxH6tlnY5R"
      },
      "source": [
        "####Menampilkan summary statistik\n",
        "\n",
        "Fungsi `describe()` digunakan untuk menampilkan summary statistik dari seluruh kolom.\n",
        "\n",
        "Jangan lupa memanggil fungsi `show()` untuk menampilkan hasilnya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_WjXzh-ncSu"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWykQ_0KnpLT"
      },
      "source": [
        "Untuk menampilkan statistik dari salah satu kolom saja, gunakan nama kolom yang akan ditampilkan sebagai parameter. Misalnya `describe('column1')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feq2JOTOn0dq"
      },
      "source": [
        "df.describe(\"density\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYDiKb4Rn_0c"
      },
      "source": [
        "###2.2 Filtering\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita dapat melakukan filtering terhadap spark dataframe, berdasar kolom atau baris"
      ],
      "metadata": {
        "id": "Y3BTu53eBAqp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-om-OrZquQQo"
      },
      "source": [
        "####Memilih kolom tertentu\n",
        "\n",
        "Untuk menampilkan kolom tertentu, digunakan fungsi `select('nama_kolom')`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdQzWlmvoJXv"
      },
      "source": [
        "df.select(\"province\").show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1x_c-b9pcWc"
      },
      "source": [
        "Untuk memilih beberapa kolom, gunakan tanda koma sebagai pemisah"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQbyEyDCpekb"
      },
      "source": [
        "df.select(\"province\",\"density\").show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghJrZSCpoo7C"
      },
      "source": [
        "####Memilih records / baris\n",
        "\n",
        "Untuk memilih baris dengan kondisi tertentu, gunakan fungsi `filter(<kondisi>)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwTmG_QyoQHM"
      },
      "source": [
        "df.filter(df.density > 1000).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4790g0r7GK"
      },
      "source": [
        "Untuk menggunakan kondisi berupa operasi string, dapat digunakan fungsi-fungsi dari `pyspark.sql.Column` yang terkait string, misalnya `contains()`, `startswith()`, `endswith()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VL5e4DFofWH"
      },
      "source": [
        "df.filter(df.province.contains('TENGGARA')).show(5)\n",
        "df.filter(df.province.startswith('SU')).show(10)\n",
        "df.filter(df.province.endswith('BARAT')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u7x8thvs2yp"
      },
      "source": [
        "Tersedia juga fungsi `like()` yang serupa dengan SQL statement *like*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3qDU-rYs7kc"
      },
      "source": [
        "df.filter(df.province.like('SU%')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWaHUzxHtSeg"
      },
      "source": [
        "Atau dapat juga menggunakan regex, dengan fungsi `rlike()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVjEkO-vtUtC"
      },
      "source": [
        "df.filter(df.province.rlike('[A-Z]*TA$')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "114ZKh-PuArH"
      },
      "source": [
        "Dapat juga menggunakan filter berdasar list, dengan fungsi `isin()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FNg0NkIt-2g"
      },
      "source": [
        "df.filter(df.timezone.isin('WIT','WITA')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.province.isin('BALI','PAPUA')).show()"
      ],
      "metadata": {
        "id": "YKWMWATGW9oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_zzv1_aYXBe"
      },
      "source": [
        "Untuk menggunakan beberapa kondisi sekaligus, menggunakan tanda `&` untuk AND dan `|` untuk OR, dengan masing-masing kondisi dilingkupi tanda kurung `()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dAZ7aDoYalg"
      },
      "source": [
        "df.filter((df.timezone.isin('WIT','WITA')) & (df.year == 2013)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZb03bk5QKwo"
      },
      "source": [
        "###2.3 Unique value\n",
        "\n",
        "Untuk menampilkan nilai unik dari dataframe, digunakan fungsi `distinct()`.\n",
        "\n",
        "Nilai unik di sini adalah kombinasi nilai dari seluruh kolom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAIvwxbpQbj-"
      },
      "source": [
        "df.distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_2VKvj1QnVc"
      },
      "source": [
        "Untuk menampilkan nilai unik dari kolom tertentu, tulis nama kolom yang dimaksud sebagai parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjlP69w-QqaV"
      },
      "source": [
        "df.select('timezone').distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('year','timezone').distinct().show()"
      ],
      "metadata": {
        "id": "VmmswyxeX_ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Menghapus duplikasi data"
      ],
      "metadata": {
        "id": "P3Uie0m1uVeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk menghapus record duplikat, gunakan `dropDuplicates(subset)` atau `drop_duplicates(subset)`."
      ],
      "metadata": {
        "id": "65nP6e1lgTFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropDuplicates(['province', 'timezone']).show(100)"
      ],
      "metadata": {
        "id": "5feLEyAJuZPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcLgEXz-PlvN"
      },
      "source": [
        "###2.4 Agregasi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0kFRECNRqBL"
      },
      "source": [
        "####Group by column\n",
        "\n",
        "Untuk mengelompokkan berdasar kolom, gunakan perintah `groupBy('nama_kolom')`\n",
        "\n",
        "Untuk mengelompokkan berdasar lebih dari 1 kolom, gunakan tanda koma sebagai pemisah nama kolom."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"timezone\")"
      ],
      "metadata": {
        "id": "oVa4FSWws8Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"timezone\",\"year\")"
      ],
      "metadata": {
        "id": "IKsy1iphtWwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perintah `groupBy()` menghasilkan obyek `GroupedData` yang belum bisa ditampilkan.\n",
        "\n",
        "Biasanya setelah pengelompokan, kita melakukan operasi sumarisasi data. Kita terapkan operasi tersebut pada objek hasil groupBy dengan memanggil fungsi yang dibutuhkan. Misalnya `count()` atau `max('namakolom')`"
      ],
      "metadata": {
        "id": "3bzjnDEQtdiI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9aXQsIWFt3O"
      },
      "source": [
        "df.groupBy('timezone').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('timezone').max('density').show()"
      ],
      "metadata": {
        "id": "0vuMZmwCZ10Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kr85hEQJl8a"
      },
      "source": [
        "Kita juga bisa menggunakan fungsi `agg()` untuk melakukan agregasi. Terutama jika kita ingin melakukan lebih dari 1 operasi agregat.\n",
        "\n",
        "Kita bisa menggunakan fungsi `alias()` untuk memberi nama kolom hasil agregasi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d2I29cpHV_Q"
      },
      "source": [
        "df.groupBy(\"timezone\").agg(F.max('density')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"timezone\").agg(F.avg('density').alias('avg_density'), \\\n",
        "                           F.min('density').alias('min_density'), \\\n",
        "                           F.max('density').alias('max_density')).show()"
      ],
      "metadata": {
        "id": "HcOH3Ry-aYry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cci7rwBA0XiD"
      },
      "source": [
        "####Order By"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LMTfZnTiZMI"
      },
      "source": [
        "df.groupBy(\"timezone\") \\\n",
        "  .mean(\"density\") \\\n",
        "  .orderBy(\"timezone\",ascending=False).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD-e2D1X0lKq"
      },
      "source": [
        "####Agregasi dengan filter / kondisi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBy0oEPGhQAL"
      },
      "source": [
        "df.groupBy(\"timezone\") \\\n",
        "  .mean(\"density\") \\\n",
        "  .where(df.timezone.contains('WIT')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtQRWTYttuXh"
      },
      "source": [
        "####Filter hasil agregat (SQL stat **HAVING**)\n",
        "\n",
        "Untuk memfilter berdasar hasil agregasi (semacam perintah **HAVING** di SQL), lakukan dalam 2 langkah.\n",
        "1. Lakukan `groupBy` + `agg` dan beri nama kolom hasil agregat dengan `alias`\n",
        "2. gunakan fungsi `filter(kondisi)` pada kolom hasil agregasi."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg = df.groupBy(\"timezone\", \"province\") \\\n",
        "  .agg(F.avg(\"density\").alias(\"avg_density\")) \\\n",
        "  .where(df.timezone.contains('WIT'))\n",
        "\n",
        "df_agg.filter(df_agg.avg_density > 50).show()"
      ],
      "metadata": {
        "id": "B9VnDO8Dw3AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.5 Transformasi DataFrame"
      ],
      "metadata": {
        "id": "t_PLmohQyBSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Kolom baru berupa nilai konstan\n",
        "\n",
        "Untuk menambahkan kolom baru ke dalam dataframe, kita bisa menggunakan perintah `withColumn()`\n",
        "\n",
        "Sedangkan untuk menambahkan sebuah nilai konstan, kita bisa menggunakan fungsi `lit(nilai_konstan)` dari `pyspark.sql.functions`, yang berfungsi membuat kolom dari nilai literal/konstan.\n",
        "\n",
        "Misalnya kita ingin menambahkan kolom **status** yang bernilai 1"
      ],
      "metadata": {
        "id": "1Xu2MZwP0Hab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('status', F.lit(1)).show()"
      ],
      "metadata": {
        "id": "RfFvAGjqAJ3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Kolom baru dari kolom yang ada\n",
        "\n"
      ],
      "metadata": {
        "id": "1LizYZ9dBYzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('tahun-1', df.year-1).show()"
      ],
      "metadata": {
        "id": "4rh9fTgaBdsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('year', df.year-1).show()"
      ],
      "metadata": {
        "id": "RMydt8Imc2KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Kondisional\n",
        "\n",
        "Untuk menambahkan kolom berdasar beberapa kondisi, gunakan `when` dan `otherwise` (jika perlu).\n",
        "\n",
        "Perhatikan bahwa `when` yang pertama adalah fungsi dalam `pyspark.sql.functions`, sedangkan `when` yang berikutnya adalah fungsi `when` pada object kolom (`pyspark.sql.Column`)\n",
        "\n",
        "Fungsi `otherwise` adalah kondisi *else* atau kondisi selain yang disebutkan pada *when*."
      ],
      "metadata": {
        "id": "yaDbu8YSCZqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('timezone_code', F.when(df.timezone == 'WIT', 1).\n",
        "              when(df.timezone == 'WITA', 2).\n",
        "              otherwise(3)).show(50)"
      ],
      "metadata": {
        "id": "t70gfGAYBtSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52CxmiOt0dwW"
      },
      "source": [
        "###2.6 Data enrichment - Join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO7h6k6iHEXr"
      },
      "source": [
        "\n",
        "Perintah ntuk melakukan join adalah sebagai berikut :\n",
        "\n",
        "`df1.join(df2, on=[columname], how=’left’)`\n",
        "\n",
        "Where :\n",
        "-    `df1` − Dataframe1.\n",
        "-    `df2` – Dataframe2.\n",
        "-    `on` − nama kolom yang akan digunakan untuk join.\n",
        "-    `how` – type of join needs to be performed – `left`, `right`, `outer`, `inner`. Defaultnya adalah inner join.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataframe yang akan digunakan untuk join"
      ],
      "metadata": {
        "id": "1kaJk2F5E4WM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zpT210OGDMa"
      },
      "source": [
        "damdata = ((\"SUMATERA SELATAN\",2),\n",
        "(\"SULAWESI TENGAH\",2),\n",
        "(\"SULAWESI SELATAN\",2),\n",
        "(\"SUMATERA BARAT\",3),\n",
        "(\"RIAU\",3),\n",
        "(\"LAMPUNG\",3),\n",
        "(\"NUSA TENGGARA TIMUR\",4),\n",
        "(\"BENGKULU\",8),\n",
        "(\"SUMATERA UTARA\",10),\n",
        "(\"JAWA TIMUR\",12),\n",
        "(\"JAWA TENGAH\",35),\n",
        "(\"JAWA BARAT\",49))\n",
        "\n",
        "df_dam = spark.createDataFrame(damdata).toDF(\"province\", \"dam_num\")\n",
        "\n",
        "df_dam.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QN1QKE8_Dp9"
      },
      "source": [
        "Join dataframe kepadatan penduduk dengan dataframe jumlah bendungan, berdasarkan nama propinsi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BxH6wBdG7bY"
      },
      "source": [
        "df.join(df_dam, on=['province'], how='left').show(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_3UfzMTIDzg"
      },
      "source": [
        "Untuk melakukan left join, tentukan parameter `how`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yLuRcy4IEV3"
      },
      "source": [
        "df.join(df_dam, on=['province'], how='left').show(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGrlD5ashACP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Lab 04 - Pandas API on Spark"
      ],
      "metadata": {
        "id": "poqPRjlYOBjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MphOaBkfOJvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.pandas as ps\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4rCT21qPOMFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf1 = ps.range(5)\n",
        "psdf1"
      ],
      "metadata": {
        "id": "_wxhUFFiwf-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf2 = ps.DataFrame({'id': [5, 4, 3]})\n",
        "psdf2"
      ],
      "metadata": {
        "id": "ZWO4w6Z8w1D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = psdf1[psdf1.id > 5]\n",
        "psdf.spark.explain()"
      ],
      "metadata": {
        "id": "dWMC_6rXzfG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pandas-on-Spark DataFrame\n",
        "psdf = ps.DataFrame({'A': np.random.rand(5),\n",
        "                     'B': np.random.rand(5)})\n",
        "psdf"
      ],
      "metadata": {
        "id": "XkTfkzWLxdxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pandas DataFrame\n",
        "pdf = pd.DataFrame({'A': np.random.rand(5),\n",
        "                    'B': np.random.rand(5)})\n",
        "\n",
        "pdf"
      ],
      "metadata": {
        "id": "z4Vgqww5yIHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = ps.DataFrame(pdf)\n",
        "psdf"
      ],
      "metadata": {
        "id": "3xX8AScm6Tmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = ps.from_pandas(pdf)\n",
        "psdf"
      ],
      "metadata": {
        "id": "jWVsw1by0B5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psdf.plot(kind=\"line\")"
      ],
      "metadata": {
        "id": "AHelGZO_0MUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara default Pandas API on Spark mencegah operasi pada DataFrame (atau Series) yang berbeda untuk menghindari operasi yang mahal.\n",
        "\n",
        "Operasi antar DataFrame dapat diaktifkan dengan mengatur pilihan `compute.ops_on_diff_frames` ke `True`."
      ],
      "metadata": {
        "id": "a1F7ifwFxIWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps.set_option('compute.ops_on_diff_frames', True)\n",
        "(psdf1 - psdf2).sort_index()"
      ],
      "metadata": {
        "id": "wzOzlmSlw2lz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}