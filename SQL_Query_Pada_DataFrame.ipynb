{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SQL adalah salah satu bahasa populer untuk pemrosesan dan analisis data. Spark mendukung SQL untuk memproses DataFrame. Dalam latihan ini kita akan menggunakan spark SQL tanpa database.\n"
      ],
      "metadata": {
        "id": "vJbZRSkkTnAw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyqGzlLOuBpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe46554-f439-4e2a-f2a9-7a4d33faaf40"
      },
      "source": [
        "%pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJDNbI5gti9B"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inisialisasi spark session untuk berinteraksi dengan Spark cluster."
      ],
      "metadata": {
        "id": "HgEv2iNhTitC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnxed8rpt5Xs"
      },
      "source": [
        "spark = SparkSession.builder.appName('DataFrame Basics').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset"
      ],
      "metadata": {
        "id": "iHGCn-voUTzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/urfie/SparkSQL-dengan-Hive/raw/main/datasets/application_record_header.csv.gz"
      ],
      "metadata": {
        "id": "PQTqnwezHtSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load ke dataframe"
      ],
      "metadata": {
        "id": "-vEl7qY6UV4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"application_record_header.csv.gz\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "ArREYTQWHy3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sebelum menggunakan SQL, kita perlu membuat temporary table dari dataframe yang akan kita olah.\n",
        "\n",
        "Gunakan fungsi `createOrReplaceTempView(nama_tabel)` pada dataframe tersebut."
      ],
      "metadata": {
        "id": "lvgyM2iKUYS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"app_record\")"
      ],
      "metadata": {
        "id": "29qI_YUJnF_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya kita bisa menggunakan nama tabel yang sudah kita definisikan dalam SQL statement.\n",
        "\n",
        "Untuk mengeksekusi SQL statement, kita gunakan fungsi `sql(sqlstatement)` pada spark session."
      ],
      "metadata": {
        "id": "xuijaPcEUz3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select count(*) from app_record\").show()"
      ],
      "metadata": {
        "id": "mEZWLQbtnIHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from app_record limit 5\").show()"
      ],
      "metadata": {
        "id": "6fLoE47EnMtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita akan melakukan join salah satu kolom dengan data referensi dan melakukan agregasi. Sebelumnya kita buat dataframe referensi dan membuat temporary viewnya"
      ],
      "metadata": {
        "id": "BeWp1FhmLrs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata = (\n",
        "    ('Lower secondary',1),\n",
        "    ('Secondary / secondary special',2),\n",
        "    ('Academic degree',3),\n",
        "    ('Incomplete higher',4),\n",
        "    ('Higher education',5))\n",
        "\n",
        "ref_edu = spark.createDataFrame(mydata).toDF(\"NAME_EDUCATION_TYPE\", \"EDU_LEVEL\")\n",
        "ref_edu.createOrReplaceTempView(\"ref_edu\")\n",
        "spark.sql(\"select * from ref_edu\").show()"
      ],
      "metadata": {
        "id": "py7cZ3yF7INt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita lakukan join, agregat, kemudian kita simpan ke tabel"
      ],
      "metadata": {
        "id": "r4kk2JrXMRRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"SELECT edu_level, count(1) as number_of_app FROM\n",
        "              (SELECT ref_edu.EDU_LEVEL as edu_level\n",
        "                FROM app_record LEFT JOIN ref_edu\n",
        "                ON app_record.NAME_EDUCATION_TYPE=ref_edu.NAME_EDUCATION_TYPE)\n",
        "             GROUP BY edu_level SORT BY edu_level\"\"\").write.saveAsTable(name=\"aggregated_edu\", mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "IkuuNd0ynWll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita tampilkan hasilnya dengan menggunakan perintah `describe formatted`"
      ],
      "metadata": {
        "id": "qV_594qnMeOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"describe formatted aggregated_edu\").show(truncate = False)"
      ],
      "metadata": {
        "id": "ysuMyldWC59p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -l /content/spark-warehouse/aggregated_edu/"
      ],
      "metadata": {
        "id": "qk474fUWO-Rz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}